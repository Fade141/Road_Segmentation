# -*- coding: utf-8 -*-
"""Road_Segmentation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Zdmxu68AzmGfzRSNRQ_AzjC28lYUz-L_

##Team Project for Road Segmentation
Hello Everyone, this is our Road Segmentation Project for CPS 4801 with Professor Huang. Abel did all the preprocessing of this notebook and together we made the Pytorch CustomDataset. We also worked together with the professors notes on making the DeepLab v3+ model. After this, Krish Performed the post processing of the deeplab v3+. For the U Net++, we both worked on the preprocessing and Krish setup the model and trained it and generated the result.

##Downloading the Data from Kaggle
link: https://www.kaggle.com/datasets/mehmetcubukcu/commaaicomplete/data
"""

pip install segmentation-models-pytorch

import kagglehub

# Download latest version
path = kagglehub.dataset_download("mehmetcubukcu/commaaicomplete")

print("Path to dataset files:", path)

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib.colors import to_rgb
import torch
import torch.nn.functional as F
from torch.utils.data import Dataset, random_split, DataLoader
from torch.optim.lr_scheduler import ReduceLROnPlateau
from torchvision.io import decode_image
from torchvision.transforms.v2 import Compose, Resize, RandomEqualize, ToDtype
from torchvision import transforms
from pathlib import Path
from os import listdir
from tqdm import tqdm
import segmentation_models_pytorch as seg_models

"""##Attempt 1"""

path = Path('/kaggle/input/commaaicomplete/comma10k') # My path for the dataset
image_height, image_width, n_channels = 256, 256, 3
# This is my preset size for all images and masks, being (256, 256, 3)
colors = torch.from_numpy(np.float32([
    to_rgb('#402020'),
    to_rgb('#ff0000'),
    to_rgb('#808060'),
    to_rgb('#00ff66'),
    to_rgb('#cc00ff')
]) * 255.).type(torch.long)
# All this information i used from the datacard in the dataset
n_classes = colors.shape[0]
# I then set the number of classes corresponding to the number of the colors shape

data = []

for filename in listdir(path / 'imgs'):
    path_to_image = path / 'imgs' / filename
    path_to_mask = path / 'masks' / filename

    data.append((path_to_image, path_to_mask))

df = pd.DataFrame(data, columns=['path_to_image', 'path_to_mask'])
df = df[:500]

df
# I then use a Pandas DataFrame where i placed my images and masks in order
# I sliced the dataframe to get the first 500 as 9888 imgs is too much

image_transform = Compose([
    Resize(size=(image_height, image_width)),
    #RandomEqualize(p=1),
    ToDtype(dtype=torch.float, scale=True)
])
mask_transform = Compose([
    Resize(size=(image_height, image_width), interpolation=InterpolationMode.NEAREST),
    ToDtype(dtype=torch.long)
])
#Resize both to 256, 256, 3
#converted imgs to torch.float and masks to torch.long

class SegmentationData(Dataset):
    def __init__(self, df, colors, image_transform, mask_transform):
        self.df = df
        self.colors = colors
        self.image_transform = image_transform
        self.mask_transform = mask_transform

    def __getitem__(self, idx):
        path_to_image, path_to_mask = self.df.iloc[idx]

        image = decode_image(path_to_image, mode='rgb')
        colored_mask = decode_image(path_to_mask, mode='rgb')
        #image = image.unsqueeze(0)
        transformed_image = self.image_transform(image)
        transformed_colored_mask = self.mask_transform(colored_mask)

        transformed_mask = transformed_colored_mask.permute(1, 2, 0).unsqueeze(dim=2)
        transformed_mask = (transformed_mask == self.colors).sum(dim=3).argmax(dim=2)

        return transformed_image, transformed_mask

    def __len__(self):
        return self.df.shape[0]

#Experimentation result with GPT

def show_images(images, titles):
    fig, ax = plt.subplots(nrows=1, ncols=len(images), figsize=(6 * len(images), 6))
    plt.subplots_adjust(wspace=0)
    for i, (image, title) in enumerate(zip(images, titles)):
        if image.shape[0] == 1:
            ax[i].imshow(image.permute(1, 2, 0), interpolation='nearest', cmap='magma_r')
        else:
            ax[i].imshow(image.permute(1, 2, 0))
        ax[i].set_title(title)
        ax[i].axis(False)
    plt.show()

ds = SegmentationData(df, colors, image_transform, mask_transform)

image, mask = ds[2]

show_images((image, mask.unsqueeze(dim=0)), ('Sample Image', 'Sample Target'))

from torchvision import models, transforms
import torch.nn as nn
batch_size = 32

train_ds, val_ds, test_ds = random_split(ds, (0.85, 0.05, 0.1),
                                         torch.Generator().manual_seed(0))

train_dl = DataLoader(train_ds, batch_size, shuffle=True)
val_dl = DataLoader(val_ds, batch_size)
test_dl = DataLoader(test_ds, batch_size)
# I split my data into 3 parts:
# Train : 85 percent
# Validation : 5 percent
# Test : 10 percent

print(colors.shape)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

model = models.segmentation.deeplabv3_resnet50(pretrained=True)
# Use the DeepLab V3+ model pretrained CICO
model.classifier[4] = nn.Conv2d(256, 5, kernel_size=(1, 1))
# Since i dont have 21 classes like the CICO dataset pretrained, I altered the last method with 5 classes
model = model.to(device) # added to GPU
optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)
criterion = torch.nn.CrossEntropyLoss()
# Best to use this as it handles unspecified indexes.

num_epochs = 20
for epoch in range(num_epochs):
    model.train()
    total_loss = 0
    for images, masks in train_dl:
        images, masks = images.to(device), masks.to(device)

        optimizer.zero_grad()
        outputs = model(images)['out']
        outputs = F.interpolate(outputs, size=masks.shape[1:], mode='bilinear', align_corners=False)
        outputs = outputs.float()
        masks = masks.long()
        loss = criterion(outputs, masks)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()

    print(f"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss:.4f}")
# 20 epochs total using the training dataloader
# I feed the model with the images and set it as result
# i resized the output to match the mask size so the images size remains the same, could lead to issues if not
# set it to float type
# Should be set as long type for masks, but ensured as I recieved GPU Errors
# calculating the loss using criterion(the output of the image versus the mask), then used backward function to
# compute the loss and added to total_loss

model.eval()
with torch.no_grad():
    for i in range(10):
        image, mask = test_ds[i]

        if image.ndim == 2:
            image = image.unsqueeze(0)

        input_tensor = image.unsqueeze(0).to(device)
        logits = model(input_tensor)
        if isinstance(logits, dict) and 'out' in logits:
            logits = logits['out']

        probs = F.softmax(logits, dim=1)
        preds = torch.argmax(probs, dim=1)

        show_images((image, mask.unsqueeze(0), preds.cpu()),
                    ('Image', 'Ground Truth', 'Predicted'))

# Post processing fully connected CRF - helpful for boundary sharpness

from sklearn.metrics import confusion_matrix, precision_score, recall_score
import numpy as np

# Step 1: Collect all predictions and labels
all_preds = []
all_labels = []

model.eval()
with torch.no_grad():
    for images, labels in tqdm(test_dl, desc="Evaluating Metrics"):
        images = images.to("cuda")
        labels = labels.to("cuda")

        outputs = model(images)['out']
        preds = torch.argmax(outputs, dim=1)

        all_preds.append(preds.cpu().numpy().flatten())
        all_labels.append(labels.cpu().numpy().flatten())

# Step 2: Flatten arrays
all_preds = np.concatenate(all_preds)
all_labels = np.concatenate(all_labels)

# Step 3: Confusion Matrix
conf_matrix = confusion_matrix(all_labels, all_preds)
print("\nConfusion Matrix:")
print(conf_matrix)

# Step 4: Precision and Recall (macro for all classes)
precision = precision_score(all_labels, all_preds, average='macro', zero_division=0)
recall = recall_score(all_labels, all_preds, average='macro', zero_division=0)

print(f"\nPrecision (macro avg): {precision:.4f}")
print(f"Recall (macro avg): {recall:.4f}")

import torch
import numpy as np
from tqdm import tqdm
from torchvision.io import decode_image

# Set up classes
n_classes = 5  # Number of classes
class_names = ['Background', 'Road', 'Grass', 'Lane', 'Car']

# Create a list to store frequency of each class
class_pixel_counts = np.zeros(n_classes)

# Go through the dataset and calculate frequency of each class
for path_to_image, path_to_mask in tqdm(df[['path_to_image', 'path_to_mask']].values):
    # Load the mask image (this will be a grayscale image where each pixel is an index for the class)
    mask = decode_image(path_to_mask, mode='rgb')  # Assuming mask is RGB with class colors

    # Convert the mask to a tensor and reshape to (height * width, n_channels)
    mask = mask.permute(1, 2, 0).view(-1, 3)  # Flatten image to a 2D tensor

    # Identify class by color (assuming each class has a distinct color in RGB)
    for i, color in enumerate(colors):
        # Match pixels of that class (e.g., color of road or lane)
        mask_class = torch.all(mask == color, dim=1).sum().item()
        class_pixel_counts[i] += mask_class

# Now compute the weights based on the inverse of the frequency of each class
total_pixels = class_pixel_counts.sum()
class_weights = total_pixels / (n_classes * class_pixel_counts)

# Print the class weights
for class_name, weight in zip(class_names, class_weights):
    print(f"Class: {class_name}, Weight: {weight:.4f}")

# You can now use these weights in your loss function
weights = torch.tensor(class_weights, device=device)

# The class weights calculated earlier (these can be adjusted if needed)
class_weights = torch.tensor([1.0678, 34.1248, 0.3810, 6.6626, 0.7941]).float().to(device)

# Modify the criterion to include class weights
criterion = nn.CrossEntropyLoss(weight=class_weights)

# During training loop:
for epoch in range(num_epochs):
    model.train()
    total_loss = 0
    for images, masks in train_dl:
        images, masks = images.to(device), masks.to(device)

        optimizer.zero_grad()
        outputs = model(images)['out']
        outputs = F.interpolate(outputs, size=masks.shape[1:], mode='bilinear', align_corners=False)

        # Ensure the data types are correct
        outputs = outputs.float()
        masks = masks.long()

        # Compute the loss with class weights
        loss = criterion(outputs, masks)
        loss.backward()
        optimizer.step()

        total_loss += loss.item()

    print(f"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss:.4f}")

model.eval()
with torch.no_grad():
    for i in range(10):
        image, mask = test_ds[i]

        if image.ndim == 2:
            image = image.unsqueeze(0)

        input_tensor = image.unsqueeze(0).to(device)
        logits = model(input_tensor)
        if isinstance(logits, dict) and 'out' in logits:
            logits = logits['out']

        probs = F.softmax(logits, dim=1)
        preds = torch.argmax(probs, dim=1)

        show_images((image, mask.unsqueeze(0), preds.cpu()),
                    ('Image', 'Ground Truth', 'Predicted'))

"""##Attempt 2:"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib.colors import to_rgb
import torch
import torch.nn.functional as F
from torch.utils.data import Dataset, random_split, DataLoader
from torch.optim.lr_scheduler import ReduceLROnPlateau
from torchvision.io import decode_image
from torchvision.transforms.v2 import Compose, Resize, RandomEqualize, ToDtype, InterpolationMode
from pathlib import Path
from os import listdir
from tqdm import tqdm
import segmentation_models_pytorch as seg_models
import os
from PIL import Image
from torch import nn
from torchvision import datasets, transforms
from torch.utils.data import Dataset, DataLoader
import random
from sklearn.model_selection import train_test_split
import glob
import torchvision.models as models
import torch.nn as nn
import torch.optim as optim
from tqdm import tqdm
import cv2
from torchvision.utils import save_image

def show_images(images, titles):
    fig, ax = plt.subplots(nrows=1, ncols=len(images), figsize=(6 * len(images), 6))
    plt.subplots_adjust(wspace=0)
    for i, (image, title) in enumerate(zip(images, titles)):
        if image.shape[0] == 1:
            ax[i].imshow(image.permute(1, 2, 0), interpolation='nearest', cmap='magma_r')
        else:
            ax[i].imshow(image.permute(1, 2, 0))
        ax[i].set_title(title)
        ax[i].axis(False)
    plt.show()
    # Same code...

image_height, image_width, n_channels = 256, 256, 3

colors = torch.from_numpy(np.float32([
    to_rgb('#402020'),
    to_rgb('#ff0000'),
    to_rgb('#808060'),
    to_rgb('#00ff66'),
    to_rgb('#cc00ff')
]) * 255.).type(torch.long)

n_classes = colors.shape[0]

image_path = '/kaggle/input/commaaicomplete/comma10k/imgs'
mask_path = '/kaggle/input/commaaicomplete/comma10k/masks'

def load_images_and_labels(base_path):
    images = []
    for filename in os.listdir(base_path):
        filepath = os.path.join(base_path, filename)
        if filepath.endswith(".png"):
            images.append(filepath)

    return images

image_path = load_images_and_labels(image_path)
mask_path = load_images_and_labels(mask_path)

image_path = image_path[:500]
mask_path = mask_path[:500]

print("Number of images:", len(image_path))
print("Number of masks:", len(mask_path))

plt.imshow(Image.open(image_path[0]))
plt.show()
plt.imshow(Image.open(mask_path[0]))
plt.show()

mask_array = np.array(Image.open(mask_path[0]))
print(np.unique(mask_array))

# In this code, it will only retreive the img and mask if ends in .PNG

from sklearn.model_selection import train_test_split

X = image_path
y = mask_path

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# 80 / 20 split even

image_transform = Compose([
    Resize(size=(image_height, image_width)),
    #RandomEqualize(p=1),
    ToDtype(dtype=torch.float, scale=True)
])
#Image is torch.float whereas mask is torch.long for model type

mask_transform = Compose([
    Resize(size=(image_height, image_width), interpolation=InterpolationMode.NEAREST),
    ToDtype(dtype=torch.long)
])

from torchvision import transforms

image_transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),  # Converts to float tensor and normalizes to [0, 1]
])

label_transform = transforms.Compose([
    transforms.Resize((224, 224), interpolation=transforms.InterpolationMode.NEAREST),
    transforms.PILToTensor(),  # Keeps label as integers
])

from PIL import Image
from torch.utils.data import Dataset
import torchvision.transforms as transforms
import torch

class CustomDataset(Dataset):
    def __init__(self, images, labels, image_transform=None, label_transform=None):
        self.images = images
        self.labels = labels
        self.image_transform = image_transform
        self.label_transform = label_transform

        # Mapping from pixel values to class indices
        self.class_map = {
            42: 0,
            76: 1,
            90: 2,
            124: 3,
            161: 4
        }

    def __len__(self):
        return len(self.images)

    def __getitem__(self, idx):
        image_path = self.images[idx]
        label_path = self.labels[idx]
        image = Image.open(image_path).convert("RGB")
        label = Image.open(label_path).convert("L")

        if self.image_transform:
            image = self.image_transform(image)

        if self.label_transform:
            label = self.label_transform(label)

        # Map pixel values to class indices
        label = torch.squeeze(label)  # [1, H, W] -> [H, W]
        label_remapped = torch.zeros_like(label, dtype=torch.long)
        for pixel_val, class_idx in self.class_map.items():
            label_remapped[label == pixel_val] = class_idx

        return image, label_remapped



train_dataset = CustomDataset(X_train, y_train, image_transform=image_transform, label_transform=label_transform)
test_dataset = CustomDataset(X_test, y_test, image_transform=image_transform, label_transform=label_transform)

image, label = train_dataset[0]
print(image.shape)  # Should be [3, 224, 224]
print(label.shape)  # Should be [224, 224]
print(torch.unique(label))  # Should show tensor([0, 1, 2, 3, 4])

# DataLoader
train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=2)
test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=2)
# Setting Batch Size 16, perfect for size

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

model = seg_models.UnetPlusPlus(classes=5)
model = model.to(device)

# Pretrained model

criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.0001)

# Training loop
num_epochs = 20
for epoch in range(num_epochs):
    model.train()
    total_loss = 0
    for images, masks in train_loader:
        images, masks = images.to(device), masks.to(device)
        optimizer.zero_grad()
        outputs = model(images)
        loss = criterion(outputs, masks)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()

    print(f"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss:.4f}")

# Save model
torch.save(model.state_dict(), "deeplabv3_model.pth")

# Inference and visualization
def decode_segmap(label_mask, colormap):
    h, w = label_mask.shape
    rgb = np.zeros((h, w, 3), dtype=np.uint8)
    for label, color in colormap.items():
        rgb[label_mask == label] = color
    return rgb

# Not needed decode_segmap as I display next cell

model.eval()
with torch.no_grad():
    for i in range(20):
        image, mask = test_dataset[i]

        if image.ndim == 2:
            image = image.unsqueeze(0)

        input_tensor = image.unsqueeze(0).to(device)
        logits = model(input_tensor)
        if isinstance(logits, dict) and 'out' in logits:
            logits = logits['out']

        probs = F.softmax(logits, dim=1)
        preds = torch.argmax(probs, dim=1)

        show_images((image, mask.unsqueeze(0), preds.cpu()),
                    ('Image', 'Ground Truth', 'Predicted'))

from sklearn.metrics import confusion_matrix, recall_score, accuracy_score
import numpy as np
import torch

# Collect all true and predicted labels
all_preds = []
all_labels = []

model.eval()
with torch.no_grad():
    for images, masks in test_loader:  # Use your validation loader
        images, masks = images.to(device), masks.to(device)
        outputs = model(images)  # No ['out'] for UnetPlusPlus
        preds = torch.argmax(outputs, dim=1)

        all_preds.append(preds.cpu().numpy().reshape(-1))
        all_labels.append(masks.cpu().numpy().reshape(-1))

# Flatten all predictions and labels
all_preds_flat = np.concatenate(all_preds)
all_labels_flat = np.concatenate(all_labels)

# Compute metrics
acc = accuracy_score(all_labels_flat, all_preds_flat)
recall = recall_score(all_labels_flat, all_preds_flat, average=None, zero_division=0)
conf_matrix = confusion_matrix(all_labels_flat, all_preds_flat)

# Print results
print(f"Accuracy: {acc:.4f}")
for i, r in enumerate(recall):
    print(f"Recall for class {i}: {r:.4f}")
print("\nConfusion Matrix:")
print(conf_matrix)

import cv2
import torch
import numpy as np
import torch.nn.functional as F
from torchvision import transforms

# Load your trained model
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)
model.eval()

# Basic transform: resize, tensor, normalize (optional)
transform = transforms.Compose([
    transforms.ToTensor(),  # Converts (H,W,C) to (C,H,W) and scales [0,255] to [0,1]
])

# Open input video
cap = cv2.VideoCapture("/content/road_video.mp4")
fps = cap.get(cv2.CAP_PROP_FPS)
w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
out = cv2.VideoWriter("output.mp4", cv2.VideoWriter_fourcc(*'mp4v'), fps, (w, h))

while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        break

    img = cv2.resize(frame, (160, 160))  # Resize to model input size
    img = transform(img).unsqueeze(0).to(device)  # Shape: (1, 3, 150, 150)

    with torch.no_grad():
        output = model(img)  # Shape: (1, 5, 150, 150)
        output = F.interpolate(output, size=(h, w), mode="bilinear", align_corners=False)
        pred_mask = torch.argmax(output.squeeze(), dim=0).cpu().numpy()  # Shape: (H, W)

    # Convert mask to RGB colors (example: 5 classes)
    color_map = np.array([[0, 0, 0], [255, 0, 0], [0, 255, 0], [0, 0, 255], [255, 255, 0]])
    mask_rgb = np.zeros((h, w, 3), dtype=np.uint8)
    for class_idx in range(5):
        mask_rgb[pred_mask == class_idx] = color_map[class_idx]

    # Overlay mask on original frame
    overlay = cv2.addWeighted(frame, 0.7, mask_rgb, 0.3, 0)

    # Write to output
    out.write(overlay)

cap.release()
out.release()
cv2.destroyAllWindows()

